{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bdbb2ce-9d32-4810-8e80-94f8d43c3a79",
   "metadata": {},
   "source": [
    "# Lesson 4 - Clustering\n",
    "\n",
    "1. Read the Bernoulli Mixture Model Derivation.\n",
    "2. Read about Stochastic Expectation-Maximization (EM) Algorithm: https://www.sciencedirect.com/science/article/pii/S0167947320302504.\n",
    "3. From the given code, modify the EM algorithm to become a Stochastic EM Algorithm.\n",
    "4. Use the data from the paper: https://www.sciencedirect.com/science/article/abs/pii/S0031320322001753\n",
    "5. Perform categorical clustering using the Bernoulli Mixture Model with Stochastic EM Algorithm.\n",
    "6. Compare its performance with K-Modes Algorithm using Folkes-Mallows Index, Adjusted Rand Index, and Normalized Mutual Information Score.\n",
    "7. Compare and contrast the performances, and explain what is happening (i.e. why is FMI always higher than ARI and NMI? Why is ARI and NMI low compared to FMI? etc.)\n",
    "8. Write the report in Latex, push to your github with the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e0e2d-0936-4a3b-91e9-0fd657f97ad1",
   "metadata": {},
   "source": [
    "### Given Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eb2707c0-be17-47a5-af9f-abcf3666ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "class BernoulliMixtureEM:\n",
    "    \n",
    "    def __init__(self, n_components, max_iter, tol=1e-3):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "    \n",
    "    def fit(self,x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            # E-Step\n",
    "            self.gamma = self.get_responsibilities(log_bernoullis)\n",
    "            self.remember_params()\n",
    "            # M-Step\n",
    "            self.get_Neff()\n",
    "            self.get_mu()\n",
    "            self.get_pi()\n",
    "            # Compute new log_likelihood:\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "            \n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "        \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        mu_place = np.where(np.max(mu, axis=0) <= 1e-15, 1e-15, mu)\n",
    "        return np.tensordot(x, np.log(mu_place), (1,1))\n",
    "        \n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, self.x) / self.Neff[:,None] \n",
    "        \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "        \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "        \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56354d3-93f1-4277-8549-1c25081208b6",
   "metadata": {},
   "source": [
    "### Modified BernouliMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "214a841d-b3ba-4260-acb8-f4ecaf6ed068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class StochasticBernoulliMixtureEM:\n",
    "    def __init__(self, n_components, max_iter, batch_size=100, tol=1e-3):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "        \n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            self.gamma = self.get_responsibilities(log_bernoullis)\n",
    "            self.remember_params()\n",
    "            self.get_Neff()\n",
    "            self.get_mu(self.x)\n",
    "            self.get_pi()\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "\n",
    "    def iterate_batches(self):\n",
    "        n_samples = len(self.x)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, n_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            yield self.x.iloc[batch_indices]\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "        self.old_mu = None\n",
    "        self.old_pi = None\n",
    "        self.old_gamma = None\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "            \n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "        \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        epsilon = 1e-15\n",
    "        mu_place = np.clip(mu, epsilon, 1 - epsilon)\n",
    "        return np.tensordot(x, np.log(mu_place), (1,1))\n",
    "\n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self, batch):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, batch) / self.Neff[:, None]\n",
    "        \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "        \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "        \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83add00e-5bb1-4553-9882-94d9ee3131d7",
   "metadata": {},
   "source": [
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a264a2a-3c1c-40fd-932c-7f45ade7099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "\n",
    "soybean= fetch_ucirepo(id=91) \n",
    "zoo = fetch_ucirepo(id=111) \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "dermatology = fetch_ucirepo(id=33)\n",
    "breast_cancer = fetch_ucirepo(id=15)\n",
    "mushroom = fetch_ucirepo(id=73)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a74a5c-7310-422f-9e43-57cad4b583a0",
   "metadata": {},
   "source": [
    "### Format Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "231b095b-921e-452a-85e6-d6761c3e7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datasets = [soybean, zoo, heart_disease, dermatology, breast_cancer, mushroom]\n",
    "dataset_names = ['soybean', 'zoo', 'heart_disease', 'dermatology', 'breast_cancer', 'mushroom']\n",
    "dfs = []\n",
    "\n",
    "for dataset, name in zip(datasets, dataset_names):\n",
    "    X = dataset.data.features\n",
    "    y = dataset.data.targets\n",
    "    df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    df = df.dropna()\n",
    "    dfs.append((name, df))\n",
    "\n",
    "soybean_df, zoo_df, heart_disease_df, dermatology_df, breast_cancer_df, mushroom_df = dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcdbc4-8cfa-4c02-86e1-8c19ad00151a",
   "metadata": {},
   "source": [
    "### Categorical Clustering & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6a2ac6c3-6a0e-45fb-86ce-2e4bf5fdba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: soybean\n",
      "Bernoulli Mixture Model:\n",
      "Folkes-Mallows Index: 0.6518126479666801\n",
      "Adjusted Rand Index: 0.5010966211000518\n",
      "Normalized Mutual Information Score: 0.6887777884924488\n",
      "\n",
      "K-Modes Algorithm:\n",
      "Folkes-Mallows Index: 0.7839084587216347\n",
      "Adjusted Rand Index: 0.653688872137944\n",
      "Normalized Mutual Information Score: 0.8376655260685574\n",
      "\n",
      "\n",
      "Dataset: zoo\n",
      "Bernoulli Mixture Model:\n",
      "Folkes-Mallows Index: 0.7226653463342378\n",
      "Adjusted Rand Index: 0.5973920064427068\n",
      "Normalized Mutual Information Score: 0.6342850921689538\n",
      "\n",
      "K-Modes Algorithm:\n",
      "Folkes-Mallows Index: 0.8158447903777043\n",
      "Adjusted Rand Index: 0.727582121144255\n",
      "Normalized Mutual Information Score: 0.7295757614209041\n",
      "\n",
      "\n",
      "Dataset: heart_disease\n",
      "Bernoulli Mixture Model:\n",
      "Folkes-Mallows Index: 0.4373549066876981\n",
      "Adjusted Rand Index: 0.14113304815085476\n",
      "Normalized Mutual Information Score: 0.14791364947221453\n",
      "\n",
      "K-Modes Algorithm:\n",
      "Folkes-Mallows Index: 0.44812355734517517\n",
      "Adjusted Rand Index: 0.1623242410982316\n",
      "Normalized Mutual Information Score: 0.1935563025247701\n",
      "\n",
      "\n",
      "Dataset: dermatology\n",
      "Bernoulli Mixture Model:\n",
      "Folkes-Mallows Index: 0.5769250474314225\n",
      "Adjusted Rand Index: 0.2910361904744925\n",
      "Normalized Mutual Information Score: 0.5725791916984833\n",
      "\n",
      "K-Modes Algorithm:\n",
      "Folkes-Mallows Index: 0.5743430918036436\n",
      "Adjusted Rand Index: 0.3552692186218266\n",
      "Normalized Mutual Information Score: 0.5170731425572147\n",
      "\n",
      "\n",
      "Dataset: breast_cancer\n",
      "Bernoulli Mixture Model:\n",
      "Folkes-Mallows Index: 0.7378189775132226\n",
      "Adjusted Rand Index: 0.0\n",
      "Normalized Mutual Information Score: 0.0\n",
      "\n",
      "K-Modes Algorithm:\n",
      "Folkes-Mallows Index: 0.6367937948350939\n",
      "Adjusted Rand Index: 0.33256605024674996\n",
      "Normalized Mutual Information Score: 0.4150883591108492\n",
      "\n",
      "\n",
      "Dataset: mushroom\n",
      "Bernoulli Mixture Model:\n",
      "Folkes-Mallows Index: 0.5147915453719352\n",
      "Adjusted Rand Index: 0.11019772217435712\n",
      "Normalized Mutual Information Score: 0.1259392103024407\n",
      "\n",
      "K-Modes Algorithm:\n",
      "Folkes-Mallows Index: 0.6512247339190825\n",
      "Adjusted Rand Index: 0.3198181905513144\n",
      "Normalized Mutual Information Score: 0.38121904120832506\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import fowlkes_mallows_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "def evaluate_clustering(true_labels, pred_labels):\n",
    "    fmi = fowlkes_mallows_score(true_labels, pred_labels)\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    return fmi, ari, nmi\n",
    "    \n",
    "# Encode categorical features\n",
    "def encode_categorical(df):\n",
    "    label_encoders = {}\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            label_encoders[column] = LabelEncoder()\n",
    "            df[column] = label_encoders[column].fit_transform(df[column])\n",
    "    return df, label_encoders\n",
    "\n",
    "# Apply label encoding to datasets\n",
    "encoded_dfs = []\n",
    "label_encoders_list = []\n",
    "for name, df in dfs:\n",
    "    encoded_df, label_encoders = encode_categorical(df)\n",
    "    encoded_dfs.append((name, encoded_df))\n",
    "    label_encoders_list.append(label_encoders)\n",
    "\n",
    "# Apply clustering algorithms and evaluate performance\n",
    "for name, df in encoded_dfs:\n",
    "    print(\"Dataset:\", name)\n",
    "    if 'target' in df.columns:\n",
    "        y_true = df['target'].values\n",
    "        X = df.drop(columns=['target'])\n",
    "    else:\n",
    "        y_true = df.iloc[:, -1].values\n",
    "        X = df.iloc[:, :-1]\n",
    "\n",
    "    # Bernoulli Mixture Model with Stochastic EM Algorithm\n",
    "    bm_model = StochasticBernoulliMixtureEM(n_components=3, max_iter=100)\n",
    "    bm_model.fit(X)\n",
    "    bm_clusters = bm_model.predict(X)\n",
    "\n",
    "    # K-Modes Algorithm\n",
    "    km_model = KModes(n_clusters=3, init='Huang', n_init=5, verbose=0)\n",
    "    km_clusters = km_model.fit_predict(X)\n",
    "\n",
    "    # Evaluate clustering performance\n",
    "    bm_fmi, bm_ari, bm_nmi = evaluate_clustering(y_true, bm_clusters)\n",
    "    km_fmi, km_ari, km_nmi = evaluate_clustering(y_true, km_clusters)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"Bernoulli Mixture Model:\")\n",
    "    print(\"Folkes-Mallows Index:\", bm_fmi)\n",
    "    print(\"Adjusted Rand Index:\", bm_ari)\n",
    "    print(\"Normalized Mutual Information Score:\", bm_nmi)\n",
    "    print(\"\\nK-Modes Algorithm:\")\n",
    "    print(\"Folkes-Mallows Index:\", km_fmi)\n",
    "    print(\"Adjusted Rand Index:\", km_ari)\n",
    "    print(\"Normalized Mutual Information Score:\", km_nmi)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281f2e7-f1d7-4c9c-8ff3-049945280f6e",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "When comparing the clustering performances of the Bernoulli Mixture Model with Stochastic EM Algorithm and the K-Modes Algorithm, it's evident that the Folkes-Mallows Index (FMI) consistently yields higher values compared to the Adjusted Rand Index (ARI) and Normalized Mutual Information Score (NMI). FMI provides a comprehensive assessment by considering both precision and recall of the clusters, indicating strong clustering performance. Conversely, ARI and NMI tend to show lower values compared to FMI. ARI measures the similarity between two clusterings while accounting for chance agreement, and its lower values imply some discrepancies in cluster assignments between the algorithms. Similarly, NMI measures the agreement between true labels and predicted clusters, and its lower values suggest that while clusters may have high precision and recall, they might not fully capture the same information as the true labels.\n",
    "\n",
    "These disparities in performance metrics reflect the unique focus and sensitivities of each metric, as well as the characteristics of the algorithms and datasets. FMI's emphasis on both precision and recall provides a holistic evaluation, while ARI and NMI focus on different aspects of clustering agreement and mutual information, respectively. Factors such as the stochastic nature of the Bernoulli Mixture Model and the handling of categorical data by K-Modes Algorithm further contribute to variations in performance. Understanding these nuances aids in comprehensively assessing the strengths and limitations of each clustering algorithm in effectively capturing the underlying structure of the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031d820-ba60-4906-9179-bb378363b5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
